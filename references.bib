
@inproceedings{bird_effectiveness_1993,
	address = {Tokyo Japan},
	title = {The effectiveness of decoupling},
	isbn = {978-0-89791-600-4},
	url = {https://dl.acm.org/doi/10.1145/165939.165952},
	doi = {10.1145/165939.165952},
	language = {en},
	urldate = {2023-04-26},
	booktitle = {Proceedings of the 7th international conference on {Supercomputing}},
	publisher = {ACM},
	author = {Bird, Peter L. and Rawsthorne, Alasdair and Topham, Nigel P.},
	month = aug,
	year = {1993},
	keywords = {next},
	pages = {47--56},
}

@inproceedings{raman_parallel-stage_2008,
	address = {Boston MA USA},
	title = {Parallel-stage decoupled software pipelining},
	isbn = {978-1-59593-978-4},
	url = {https://dl.acm.org/doi/10.1145/1356058.1356074},
	doi = {10.1145/1356058.1356074},
	language = {en},
	urldate = {2023-04-26},
	booktitle = {Proceedings of the 6th annual {IEEE}/{ACM} international symposium on {Code} generation and optimization},
	publisher = {ACM},
	author = {Raman, Easwaran and Ottoni, Guilherme and Raman, Arun and Bridges, Matthew J. and August, David I.},
	month = apr,
	year = {2008},
	keywords = {next},
	pages = {114--123},
}

@inproceedings{ham_desc_2015,
	address = {Waikiki Hawaii},
	title = {{DeSC}: decoupled supply-compute communication management for heterogeneous architectures},
	isbn = {978-1-4503-4034-2},
	shorttitle = {{DeSC}},
	url = {https://dl.acm.org/doi/10.1145/2830772.2830800},
	doi = {10.1145/2830772.2830800},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {Proceedings of the 48th {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Ham, Tae Jun and Aragón, Juan L. and Martonosi, Margaret},
	month = dec,
	year = {2015},
	keywords = {not-cache(com queues), not-reactive, run-ahead},
	pages = {191--203},
}

@incollection{lengauer_limitation_1997,
	address = {Berlin, Heidelberg},
	title = {A limitation study into access decoupling},
	volume = {1300},
	isbn = {978-3-540-63440-9 978-3-540-69549-3},
	url = {http://link.springer.com/10.1007/BFb0002859},
	language = {en},
	urldate = {2023-04-26},
	booktitle = {Euro-{Par}'97 {Parallel} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jones, G. P. and Topham, N. P.},
	editor = {Lengauer, Christian and Griebl, Martin and Gorlatch, Sergei},
	year = {1997},
	doi = {10.1007/BFb0002859},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1102--1111},
}

@article{weiser_program_1984,
	title = {Program {Slicing}},
	volume = {SE-10},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5010248/},
	doi = {10.1109/TSE.1984.5010248},
	number = {4},
	urldate = {2023-04-25},
	journal = {IEEE Transactions on Software Engineering},
	author = {Weiser, Mark},
	month = jul,
	year = {1984},
	pages = {352--357},
}

@inproceedings{jimborean_fix_2014,
	address = {Orlando FL USA},
	title = {Fix the code. {Don}'t tweak the hardware: {A} new compiler approach to {Voltage}-{Frequency} scaling},
	isbn = {978-1-4503-2670-4},
	shorttitle = {Fix the code. {Don}'t tweak the hardware},
	url = {https://dl.acm.org/doi/10.1145/2544137.2544161},
	doi = {10.1145/2544137.2544161},
	language = {en},
	urldate = {2023-04-24},
	booktitle = {Proceedings of {Annual} {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {ACM},
	author = {Jimborean, Alexandra and Koukos, Konstantinos and Spiliopoulos, Vasileios and Black-Schaffer, David and Kaxiras, Stefanos},
	month = feb,
	year = {2014},
	pages = {262--272},
}

@inproceedings{basak_analysis_2019,
	address = {Washington, DC, USA},
	title = {Analysis and {Optimization} of the {Memory} {Hierarchy} for {Graph} {Processing} {Workloads}},
	isbn = {978-1-72811-444-6},
	url = {https://ieeexplore.ieee.org/document/8675225/},
	doi = {10.1109/HPCA.2019.00051},
	urldate = {2023-01-26},
	booktitle = {2019 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Basak, Abanti and Li, Shuangchen and Hu, Xing and Oh, Sang Min and Xie, Xinfeng and Zhao, Li and Jiang, Xiaowei and Xie, Yuan},
	month = feb,
	year = {2019},
	keywords = {L2, hw, reactive},
	pages = {373--386},
}

@inproceedings{ainsworth_graph_2016,
	address = {Istanbul Turkey},
	title = {Graph {Prefetching} {Using} {Data} {Structure} {Knowledge}},
	isbn = {978-1-4503-4361-9},
	url = {https://dl.acm.org/doi/10.1145/2925426.2926254},
	doi = {10.1145/2925426.2926254},
	language = {en},
	urldate = {2023-04-22},
	booktitle = {Proceedings of the 2016 {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Ainsworth, Sam and Jones, Timothy M.},
	month = jun,
	year = {2016},
	pages = {1--11},
}

@inproceedings{nesbit_data_2004,
	address = {Madrid, Spain},
	title = {Data {Cache} {Prefetching} {Using} a {Global} {History} {Buffer}},
	isbn = {978-0-7695-2053-7},
	url = {http://ieeexplore.ieee.org/document/1410068/},
	doi = {10.1109/HPCA.2004.10030},
	urldate = {2023-04-22},
	booktitle = {10th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA}'04)},
	publisher = {IEEE},
	author = {Nesbit, K.J. and Smith, J.E.},
	year = {2004},
	pages = {96--96},
}

@article{smith_decoupled_1982,
	title = {Decoupled access/execute computer architectures},
	volume = {10},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/1067649.801719},
	doi = {10.1145/1067649.801719},
	abstract = {An architecture for improving computer performance is presented and discussed. The main feature of the architecture is a high degree of decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues. A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams. This paper emphasizes implementation features that remove this burden from the programmer. Performance comparisons with a conventional scalar architecture are given, and these show that considerable performance gains are possible.
            Single instruction stream versions, both physical and conceptual, are discussed with the primary goal of minimizing the differences with conventional architectures. This would allow known compilation and programming techniques to be used. Finally, the problem of deadlock in such a system is discussed, and one possible solution is given.},
	language = {en},
	number = {3},
	urldate = {2023-04-21},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Smith, James E.},
	month = apr,
	year = {1982},
	keywords = {run-ahead},
	pages = {112--119},
}

@unpublished{sotiropoulos_2023_nodate,
	title = {2023 w16},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2023_nodate-1,
	title = {2023 w15},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{hashemi_continuous_2016,
	address = {Taipei, Taiwan},
	title = {Continuous runahead: {Transparent} hardware acceleration for memory intensive workloads},
	isbn = {978-1-5090-3508-3},
	shorttitle = {Continuous runahead},
	url = {http://ieeexplore.ieee.org/document/7783764/},
	doi = {10.1109/MICRO.2016.7783764},
	urldate = {2023-04-10},
	booktitle = {2016 49th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	publisher = {IEEE},
	author = {Hashemi, Milad and Mutlu, Onur and Patt, Yale N.},
	month = oct,
	year = {2016},
	pages = {1--12},
}

@inproceedings{dahlgren_effectiveness_1995,
	address = {Raleigh, NC, USA},
	title = {Effectiveness of hardware-based stride and sequential prefetching in shared-memory multiprocessors},
	isbn = {978-0-8186-6445-8},
	url = {http://ieeexplore.ieee.org/document/386554/},
	doi = {10.1109/HPCA.1995.386554},
	urldate = {2023-04-10},
	booktitle = {Proceedings of 1995 1st {IEEE} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Dahlgren, F. and Stenstrom, P.},
	year = {1995},
	pages = {68--77},
}

@inproceedings{carlson_load_2015,
	address = {Portland Oregon},
	title = {The load slice core microarchitecture},
	isbn = {978-1-4503-3402-0},
	url = {https://dl.acm.org/doi/10.1145/2749469.2750407},
	doi = {10.1145/2749469.2750407},
	language = {en},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Carlson, Trevor E. and Heirman, Wim and Allam, Osman and Kaxiras, Stefanos and Eeckhout, Lieven},
	month = jun,
	year = {2015},
	pages = {272--284},
}

@inproceedings{ayers_asmdb_2019,
	address = {Phoenix Arizona},
	title = {{AsmDB}: understanding and mitigating front-end stalls in warehouse-scale computers},
	isbn = {978-1-4503-6669-4},
	shorttitle = {{AsmDB}},
	url = {https://dl.acm.org/doi/10.1145/3307650.3322234},
	doi = {10.1145/3307650.3322234},
	language = {en},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 46th {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Ayers, Grant and Nagendra, Nayana Prasad and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
	month = jun,
	year = {2019},
	pages = {462--473},
}

@inproceedings{lattner_llvm_2004,
	address = {San Jose, CA, USA},
	title = {{LLVM}: {A} compilation framework for lifelong program analysis \& transformation},
	isbn = {978-0-7695-2102-2},
	shorttitle = {{LLVM}},
	url = {http://ieeexplore.ieee.org/document/1281665/},
	doi = {10.1109/CGO.2004.1281665},
	urldate = {2023-04-02},
	booktitle = {International {Symposium} on {Code} {Generation} and {Optimization}, 2004. {CGO} 2004.},
	publisher = {IEEE},
	author = {Lattner, C. and Adve, V.},
	year = {2004},
	pages = {75--86},
}

@unpublished{sotiropoulos_2023_nodate-2,
	title = {2023 w12},
	author = {Sotiropoulos, Konstantinos},
}

@techreport{page_pagerank_1998,
	address = {Stanford InfoLab},
	title = {The pagerank citation ranking: {Bringing} order to the web},
	url = {http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf},
	author = {Page, Larry and Brin, Sergey},
	month = jan,
	year = {1998},
}

@article{hennessy_new_2019,
	title = {A new golden age for computer architecture},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3282307},
	doi = {10.1145/3282307},
	abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
	language = {en},
	number = {2},
	urldate = {2023-03-23},
	journal = {Communications of the ACM},
	author = {Hennessy, John L. and Patterson, David A.},
	month = jan,
	year = {2019},
	pages = {48--60},
}

@unpublished{sotiropoulos_2023_nodate-3,
	title = {2023 w11},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_w48_isp_nodate,
	title = {2022\_w48\_ISP},
	author = {Sotiropoulos, Konstantinos},
}

@book{bouchez_tichadou_ssa-based_2023,
	address = {Cham},
	edition = {1st ed. 2022},
	title = {{SSA}-based {Compiler} {Design}},
	isbn = {978-3-030-80514-2},
	abstract = {This book provides readers with a single-source reference to static-single assignment(SSA)-based compiler design. It is the first (and up to now only) book that coversin a deep and comprehensive way how an optimizing compiler can be designed usingthe SSA form. After introducing vanilla SSA and its main properties, the authorsdescribe several compiler analyses and optimizations under this form. They illustratehow compiler design can be made simpler and more efficient, thanks to the SSA form.This book also serves as a valuable text/reference for lecturers, making the teaching ofcompilers simpler and more effective. Coverage also includes advanced topics, such ascode generation, aliasing, predication and more, making this book a valuable referencefor advanced students and practicing engineers},
	language = {eng},
	publisher = {Springer Nature Switzerland AG},
	editor = {Bouchez Tichadou, Florent and Rastello, Fabrice},
	year = {2023},
}

@article{kennedy_partial_1999,
	title = {Partial redundancy elimination in {SSA} form},
	volume = {21},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/319301.319348},
	doi = {10.1145/319301.319348},
	abstract = {The SSAPRE algorithm for performing partial redundancy elimination based entirely on SSA form is presented. The algorithm is formulated based on a new conceptual framework, the factored redundancy graph, for analyzing redundancy, and representes the first sparse approach to the classical problem and on methods for its solution. With the algorithm description, theorems and their proofs are given showing that the algorithm produces the best possible code by the criteria of computational optimality and lifetime optimality of the introduced temporaries. In addition to the base algorithm, a practical implementation of SSAPRE that exhibits additional compile-time efficiencies is described. In closing, measurement statistics are provided that characterize the instances of the partial   redundancy problem from a set of benchmark programs and compare optimization time spent by an implementation of SSAPRE aganist a classical partial redundancy elimination implementation. The data lend insight into the nature of partial redundancy elimination and demonstrate the expediency of this new approach.},
	language = {en},
	number = {3},
	urldate = {2023-03-15},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Kennedy, Robert and Chan, Sun and Liu, Shin-Ming and Lo, Raymond and Tu, Peng and Chow, Fred},
	month = may,
	year = {1999},
	pages = {627--676},
}

@article{george_iterated_1996,
	title = {Iterated register coalescing},
	volume = {18},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/229542.229546},
	doi = {10.1145/229542.229546},
	abstract = {An important function of any register allocator is to target registers so as to eliminate copy instructions. Graph-coloring register allocation is an elegant approach to this problem. If the source and destination of a move instruction do not interfere, then their nodes can be coalesced in the interference graph. Chaitin's coalescing heuristic could make a graph uncolorable (i.e., introduce spills); Briggs et al. demonstrated a conservative coalescing heuristic that preserves colorability. But Briggs's algorithm is
              too
              conservative and leaves too many move instructions in our programs. We show how to interleave coloring reductions with Briggs's coalescing heuristic, leading to an algorithm that is safe but much more aggressive.},
	language = {en},
	number = {3},
	urldate = {2023-03-12},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {George, Lal and Appel, Andrew W.},
	month = may,
	year = {1996},
	pages = {300--324},
}

@article{kam_monotone_1977,
	title = {Monotone data flow analysis frameworks},
	volume = {7},
	issn = {0001-5903, 1432-0525},
	url = {http://link.springer.com/10.1007/BF00290339},
	doi = {10.1007/BF00290339},
	language = {en},
	number = {3},
	urldate = {2023-03-09},
	journal = {Acta Informatica},
	author = {Kam, John B. and Ullman, Jeffrey D.},
	year = {1977},
	pages = {305--317},
}

@article{wegman_constant_1991,
	title = {Constant propagation with conditional branches},
	volume = {13},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/103135.103136},
	doi = {10.1145/103135.103136},
	abstract = {Constant propagation is a well-known global flow analysis problem. The goal of constant propagation is to discover values that are constant on all possible executions of a program and to propagate these constant values as far foward through the program as possible. Expressions whose operands are all constants can be evaluated at compile time and the results propagated further. Using the algorithms presented in this paper can produce smaller and faster compiled programs. The same algorithms can be used for other kinds of analyses (e.g., type of determination). We present four algorithms in this paper, all
              conservitive
              in the sense that all constants may not be found, but each constant found is constant over all possible executions of the program. These algorithms are among the simplest, fastest, and most powerful global constant propagation algorithms known. We also present a new algorithm that performs a form of interprocedural data flow analysis in which aliasing information is gathered in conjunction with constant progagation. Several variants of this algorithm are considered.},
	language = {en},
	number = {2},
	urldate = {2023-02-19},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Wegman, Mark N. and Zadeck, F. Kenneth},
	month = apr,
	year = {1991},
	pages = {181--210},
}

@article{cytron_efficiently_1991,
	title = {Efficiently computing static single assignment form and the control dependence graph},
	volume = {13},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/115372.115320},
	doi = {10.1145/115372.115320},
	language = {en},
	number = {4},
	urldate = {2023-02-07},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K. and Wegman, Mark N. and Zadeck, F. Kenneth},
	month = oct,
	year = {1991},
	pages = {451--490},
}

@article{lengauer_fast_1979,
	title = {A fast algorithm for finding dominators in a flowgraph},
	volume = {1},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/357062.357071},
	doi = {10.1145/357062.357071},
	abstract = {A fast algorithm for finding dominators in a flowgraph is presented. The algorithm uses 
depth-first search and an efficient method of computing functions defined on paths in trees. A simple implementation of the algorithm runs in
              O
              (
              m
              log
              n
              ) time, where
              m
              is the number of edges and
              n
              is the number of vertices in the problem graph. A more sophisticated implementation runs in
              O
              (
              m
              α(
              m
              ,
              n
              )) time, where α(
              m
              ,
              n
              ) is a functional inverse of Ackermann's function.
            
            
              Both versions of the algorithm were implemented in Algol W, a Stanford University version of Algol, and tested on an IBM 370/168. The programs were compared with an implementation by Purdom and Moore of a straightforward
              O
              (
              mn
              )-time algorithm, and with a bit vector algorithm described by Aho and Ullman. The fast algorithm beat the straightforward algorithm and the bit vector algorithm on all but the smallest graphs tested.},
	language = {en},
	number = {1},
	urldate = {2023-01-13},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Lengauer, Thomas and Tarjan, Robert Endre},
	month = jan,
	year = {1979},
	pages = {121--141},
}

@unpublished{sotiropoulos_2023_nodate-4,
	title = {2023 w10},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2023_nodate-5,
	title = {2023 w08},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{roth_dependence_1998,
	address = {San Jose, California, United States},
	title = {Dependence based prefetching for linked data structures},
	isbn = {978-1-58113-107-9},
	url = {http://portal.acm.org/citation.cfm?doid=291069.291034},
	doi = {10.1145/291069.291034},
	language = {en},
	urldate = {2022-12-07},
	booktitle = {Proceedings of the eighth international conference on {Architectural} support for programming languages and operating systems  - {ASPLOS}-{VIII}},
	publisher = {ACM Press},
	author = {Roth, Amir and Moshovos, Andreas and Sohi, Gurindar S.},
	year = {1998},
	keywords = {not-cache(prefetch buffer), not-reactive, sw},
	pages = {115--126},
}

@inproceedings{karlsson_prefetching_1999,
	address = {Touluse, France},
	title = {A prefetching technique for irregular accesses to linked data structures},
	isbn = {978-0-7695-0550-3},
	url = {http://ieeexplore.ieee.org/document/824351/},
	doi = {10.1109/HPCA.2000.824351},
	urldate = {2023-01-16},
	booktitle = {Proceedings {Sixth} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture}. {HPCA}-6 ({Cat}. {No}.{PR00550})},
	publisher = {IEEE Comput. Soc},
	author = {Karlsson, M. and Dahlgren, F. and Stenstrom, P.},
	year = {1999},
	keywords = {L1, hw-assisted, not-reactive, sw},
	pages = {206--217},
}

@inproceedings{nguyen_pipette_2020,
	address = {Athens, Greece},
	title = {Pipette: {Improving} {Core} {Utilization} on {Irregular} {Applications} through {Intra}-{Core} {Pipeline} {Parallelism}},
	isbn = {978-1-72817-383-2},
	shorttitle = {Pipette},
	url = {https://ieeexplore.ieee.org/document/9251856/},
	doi = {10.1109/MICRO50266.2020.00056},
	urldate = {2023-01-31},
	booktitle = {2020 53rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	publisher = {IEEE},
	author = {Nguyen, Quan M. and Sanchez, Daniel},
	month = oct,
	year = {2020},
	keywords = {not-cache(com queues), not-reactive, run-ahead},
	pages = {596--608},
}

@article{manocha_graphattack_2021,
	title = {{GraphAttack}: {Optimizing} {Data} {Supply} for {Graph} {Applications} on {In}-{Order} {Multicore} {Architectures}},
	volume = {18},
	issn = {1544-3566, 1544-3973},
	shorttitle = {{GraphAttack}},
	url = {https://dl.acm.org/doi/10.1145/3469846},
	doi = {10.1145/3469846},
	abstract = {Graph structures are a natural representation of important and pervasive data. While graph applications have significant parallelism, their characteristic pointer indirect loads to neighbor data hinder scalability to large datasets on multicore systems. A scalable and efficient system must tolerate latency while leveraging data parallelism across millions of vertices. Modern Out-of-Order (OoO) cores inherently tolerate a fraction of long latencies, but become clogged when running severely memory-bound applications. Combined with large power/area footprints, this limits their parallel scaling potential and, consequently, the gains that existing software frameworks can achieve. Conversely, accelerator and memory hierarchy designs provide performant hardware specializations, but cannot support diverse application demands.
            To address these shortcomings, we present GraphAttack, a hardware-software data supply approach that accelerates graph applications on in-order multicore architectures. GraphAttack proposes compiler passes to (1) identify idiomatic long-latency loads and (2) slice programs along these loads into data Producer/ Consumer threads to map onto pairs of parallel cores. Each pair shares a communication queue; the Producer asynchronously issues long-latency loads, whose results are buffered in the queue and used by the Consumer. This scheme drastically increases memory-level parallelism (MLP) to mitigate latency bottlenecks. In equal-area comparisons, GraphAttack outperforms OoO cores, do-all parallelism, prefetching, and prior decoupling approaches, achieving a 2.87× speedup and 8.61× gain in energy efficiency across a range of graph applications. These improvements scale; GraphAttack achieves a 3× speedup over 64 parallel cores. Lastly, it has pragmatic design principles; it enhances in-order architectures that are gaining increasing open-source support.},
	language = {en},
	number = {4},
	urldate = {2023-01-25},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Manocha, Aninda and Sorensen, Tyler and Tureci, Esin and Matthews, Opeoluwa and Aragón, Juan L. and Martonosi, Margaret},
	month = dec,
	year = {2021},
	keywords = {not-cache(com queues), not-reactive, run-ahead},
	pages = {1--26},
}

@inproceedings{yang_spzip_2021,
	address = {Valencia, Spain},
	title = {{SpZip}: {Architectural} {Support} for {Effective} {Data} {Compression} {In} {Irregular} {Applications}},
	isbn = {978-1-66543-333-4},
	shorttitle = {{SpZip}},
	url = {https://ieeexplore.ieee.org/document/9499902/},
	doi = {10.1109/ISCA52012.2021.00087},
	urldate = {2022-10-02},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Yang, Yifan and Emer, Joel S. and Sanchez, Daniel},
	month = jun,
	year = {2021},
	keywords = {L2, compression, not-reactive, programmable prefetcher},
	pages = {1069--1082},
}

@phdthesis{nguyen_accelerating_2022,
	address = {Cambridge, MA, USA},
	title = {Accelerating {Irregular} {Applications} with {Pipeline} {Parallelism}},
	copyright = {Massachusetts Institute of Technology 2022. All rights reserved},
	url = {https://dspace.mit.edu/bitstream/handle/1721.1/144589/Nguyen-qmn-PhD-EECS-2022-thesis.pdf?sequence=1&isAllowed=y},
	abstract = {Irregular applications have frequent data-dependent memory accesses and control flow. They arise in many emerging and important domains, including sparse deep learning, graph analytics, and database processing. Conventional architectures cannot handle irregular applications efficiently because their techniques for improving performance, like exploiting instruction-level or data-level parallelism, are not tailored to them. Thus, continued progress in these crucial domains depends on exploring new avenues of parallelism.
Fortunately, irregular applications contain abundant but untapped pipeline parallelism: they can be divided into networks of stages. Pipelining not only exposes parallelism but also enables decoupling, which hides the latency of long events by allowing producer stages to run ahead of consumer stages. To properly decouple these applications, though, this pipeline parallelism must be exploited at fine-grain, with few operations per stage. Prior work has proposed architectures, compilers, and languages for pipelines, but focus on regular pipelines, and thus are unable to overcome several challenges of irregular applications. First, architectures need to support the efficient execution of many fine-grain pipeline stages. Second, such irregular pipelines suffer from load imbalance, as the amount of work in each stage varies rapidly as the program runs. Finally, these stages must communicate and coordinate changes in control flow.
This thesis demonstrates that exploiting fine-grain pipeline parallelism in irregular applica- tions is effective and practical. To this end, this thesis proposes two hardware architectures and a compiler: Pipette, the first architecture, reuses existing structures in modern out-of- order cores to implement load-balanced decoupled communication between stages; and Fifer, the second architecture, makes the acceleration benefits of coarse-grain reconfigurable arrays available to irregular applications. Pipette achieves gmean 1.9× speedup over a data-parallel implementation, and Fifer achieves up to 47× speedup over an out-of-order multicore while using considerably less area. Both architectures also further accelerate challenging memory accesses and resolve the load balancing and control flow challenges that are ubiquitous in irregular applications. Finally, Phloem is a compiler that makes it easy for programmers to use these architectures by producing high-performance pipeline-parallel implementations of irregular applications from serial code. Phloem automatically achieves 85\% of the performance of manually pipelined versions.},
	school = {Massachusetts Institute of Technology},
	author = {Nguyen, Quan M.},
	month = may,
	year = {2022},
	keywords = {not-cache(com queues), not-reactive, run-ahead},
}

@inproceedings{nguyen_phloem_2023,
	title = {Phloem: {Automatic} {Acceleration} of {Irregular} {Applications} with {Fine}-{Grain} {Pipeline} {Parallelism}},
	url = {https://people.csail.mit.edu/qmn/papers/nguyen_phloem_hpca_2023_preprint.pdf},
	author = {Nguyen, Quan and Sanchez, Daniel},
	month = jan,
	year = {2023},
	keywords = {not-cache(com queues), not-reactive, run-ahead},
}

@inproceedings{ayers_classifying_2020,
	address = {Lausanne Switzerland},
	title = {Classifying {Memory} {Access} {Patterns} for {Prefetching}},
	isbn = {978-1-4503-7102-5},
	url = {https://dl.acm.org/doi/10.1145/3373376.3378498},
	doi = {10.1145/3373376.3378498},
	language = {en},
	urldate = {2023-01-18},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Ayers, Grant and Litz, Heiner and Kozyrakis, Christos and Ranganathan, Parthasarathy},
	month = mar,
	year = {2020},
	keywords = {L1, not-reactive, sw},
	pages = {513--526},
}

@inproceedings{chappell_simultaneous_1999,
	address = {Atlanta, GA, USA},
	title = {Simultaneous subordinate microthreading ({SSMT})},
	isbn = {978-0-7695-0170-3},
	url = {http://ieeexplore.ieee.org/document/765950/},
	doi = {10.1109/ISCA.1999.765950},
	urldate = {2022-11-01},
	booktitle = {Proceedings of the 26th {International} {Symposium} on {Computer} {Architecture} ({Cat}. {No}.{99CB36367})},
	publisher = {IEEE Comput. Soc. Press},
	author = {Chappell, R.S. and Stark, J. and Kim, S.P. and Reinhardt, S.K. and Patt, Y.N.},
	year = {1999},
	keywords = {L1, reactive, run-ahead},
	pages = {186--195},
}

@inproceedings{mutlu_runahead_2003,
	address = {Anaheim, CA, USA},
	title = {Runahead execution: an alternative to very large instruction windows for out-of-order processors},
	isbn = {978-0-7695-1871-8},
	shorttitle = {Runahead execution},
	url = {http://ieeexplore.ieee.org/document/1183532/},
	doi = {10.1109/HPCA.2003.1183532},
	urldate = {2022-11-14},
	booktitle = {The {Ninth} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture}, 2003. {HPCA}-9 2003. {Proceedings}.},
	publisher = {IEEE Comput. Soc},
	author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y.N.},
	year = {2003},
	keywords = {L1, reactive, run-ahead},
	pages = {129--140},
}

@inproceedings{ainsworth_event-triggered_2018,
	address = {Williamsburg VA USA},
	title = {An {Event}-{Triggered} {Programmable} {Prefetcher} for {Irregular} {Workloads}},
	isbn = {978-1-4503-4911-6},
	url = {https://dl.acm.org/doi/10.1145/3173162.3173189},
	doi = {10.1145/3173162.3173189},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the {Twenty}-{Third} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Ainsworth, Sam and Jones, Timothy M.},
	month = mar,
	year = {2018},
	keywords = {L2, programmable prefetcher, reactive},
	pages = {578--592},
}

@inproceedings{talati_prodigy_2021,
	address = {Seoul, Korea (South)},
	title = {Prodigy: {Improving} the {Memory} {Latency} of {Data}-{Indirect} {Irregular} {Workloads} {Using} {Hardware}-{Software} {Co}-{Design}},
	isbn = {978-1-66542-235-2},
	shorttitle = {Prodigy},
	url = {https://ieeexplore.ieee.org/document/9407222/},
	doi = {10.1109/HPCA51647.2021.00061},
	urldate = {2022-09-27},
	booktitle = {2021 {IEEE} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Talati, Nishil and May, Kyle and Behroozi, Armand and Yang, Yichen and Kaszyk, Kuba and Vasiladiotis, Christos and Verma, Tarunesh and Li, Lu and Nguyen, Brandon and Sun, Jiawen and Morton, John Magnus and Ahmadi, Agreen and Austin, Todd and O'Boyle, Michael and Mahlke, Scott and Mudge, Trevor and Dreslinski, Ronald},
	month = feb,
	year = {2021},
	keywords = {L1, programmable prefetcher, reactive},
	pages = {654--667},
}

@article{ainsworth_software_2018,
	title = {Software {Prefetching} for {Indirect} {Memory} {Accesses}: {A} {Microarchitectural} {Perspective}},
	volume = {36},
	issn = {0734-2071, 1557-7333},
	shorttitle = {Software {Prefetching} for {Indirect} {Memory} {Accesses}},
	url = {https://dl.acm.org/doi/10.1145/3319393},
	doi = {10.1145/3319393},
	abstract = {Many modern data processing and HPC workloads are heavily memory-latency bound. A tempting proposition to solve this is software prefetching, where special non-blocking loads are used to bring data into the cache hierarchy just before being required. However, these are difficult to insert to effectively improve performance, and techniques for automatic insertion are currently limited.
            This article develops a novel compiler pass to automatically generate software prefetches for indirect memory accesses, a special class of irregular memory accesses often seen in high-performance workloads. We evaluate this across a wide set of systems, all of which gain benefit from the technique. We then evaluate the extent to which good prefetch instructions are architecture dependent and the class of programs that are particularly amenable. Across a set of memory-bound benchmarks, our automated pass achieves average speedups of 1.3× for an Intel Haswell processor, 1.1× for both an ARM Cortex-A57 and Qualcomm Kryo, 1.2× for a Cortex-72 and an Intel Kaby Lake, and 1.35× for an Intel Xeon Phi Knight’s Landing, each of which is an out-of-order core, and performance improvements of 2.1× and 2.7× for the in-order ARM Cortex-A53 and first generation Intel Xeon Phi.},
	language = {en},
	number = {3},
	urldate = {2023-03-10},
	journal = {ACM Transactions on Computer Systems},
	author = {Ainsworth, Sam and Jones, Timothy M.},
	month = aug,
	year = {2018},
	pages = {1--34},
}

@inproceedings{kim_design_2002,
	address = {San Jose California},
	title = {Design and evaluation of compiler algorithms for pre-execution},
	isbn = {978-1-58113-574-9},
	url = {https://dl.acm.org/doi/10.1145/605397.605415},
	doi = {10.1145/605397.605415},
	language = {en},
	urldate = {2023-03-10},
	booktitle = {Proceedings of the 10th international conference on {Architectural} support for programming languages and operating systems},
	publisher = {ACM},
	author = {Kim, Dongkeun and Yeung, Donald},
	month = oct,
	year = {2002},
	pages = {159--170},
}

@unpublished{sotiropoulos_2023_nodate-6,
	title = {2023 w09},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{campanoni_helix-rc_2014,
	address = {Minneapolis, MN, USA},
	title = {{HELIX}-{RC}: {An} architecture-compiler co-design for automatic parallelization of irregular programs},
	isbn = {978-1-4799-4394-4 978-1-4799-4396-8},
	shorttitle = {{HELIX}-{RC}},
	url = {http://ieeexplore.ieee.org/document/6853215/},
	doi = {10.1109/ISCA.2014.6853215},
	urldate = {2023-02-23},
	booktitle = {2014 {ACM}/{IEEE} 41st {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Campanoni, Simone and Brownell, Kevin and Kanev, Svilen and Jones, Timothy M. and Wei, Gu-Yeon and Brooks, David},
	month = jun,
	year = {2014},
	pages = {217--228},
}

@unpublished{sotiropoulos_2023_nodate-7,
	title = {2023 w07},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{bhattacharjee_translation-triggered_2017,
	address = {Xi'an China},
	title = {Translation-{Triggered} {Prefetching}},
	isbn = {978-1-4503-4465-4},
	url = {https://dl.acm.org/doi/10.1145/3037697.3037705},
	doi = {10.1145/3037697.3037705},
	language = {en},
	urldate = {2023-02-14},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Bhattacharjee, Abhishek},
	month = apr,
	year = {2017},
	pages = {63--76},
}

@article{amdahl_architecture_1964,
	title = {Architecture of the {IBM} {System}/360},
	volume = {8},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5392210},
	doi = {10.1147/rd.82.0087},
	number = {2},
	urldate = {2023-02-14},
	journal = {IBM Journal of Research and Development},
	author = {Amdahl, G. M. and Blaauw, G. A. and Brooks, F. P.},
	month = apr,
	year = {1964},
	pages = {87--101},
}

@unpublished{sotiropoulos_2023_nodate-8,
	title = {2023 w06},
	author = {Sotiropoulos, Konstantinos},
}

@article{manocha_graphfire_2023,
	title = {Graphfire: {Synergizing} {Fetch}, {Insertion}, and {Replacement} {Policies} for {Graph} {Analytics}},
	volume = {72},
	issn = {0018-9340, 1557-9956, 2326-3814},
	shorttitle = {Graphfire},
	url = {https://ieeexplore.ieee.org/document/9730090/},
	doi = {10.1109/TC.2022.3157525},
	number = {1},
	urldate = {2023-02-09},
	journal = {IEEE Transactions on Computers},
	author = {Manocha, Aninda and Aragon, Juan L. and Martonosi, Margaret},
	month = jan,
	year = {2023},
	pages = {291--304},
}

@unpublished{sotiropoulos_2023_nodate-9,
	title = {2023 w05},
	author = {Sotiropoulos, Konstantinos},
}

@article{zhang_minnow_2018,
	title = {Minnow: {Lightweight} {Offload} {Engines} for {Worklist} {Management} and {Worklist}-{Directed} {Prefetching}},
	volume = {53},
	issn = {0362-1340, 1558-1160},
	shorttitle = {Minnow},
	url = {https://dl.acm.org/doi/10.1145/3296957.3173197},
	doi = {10.1145/3296957.3173197},
	abstract = {The importance of irregular applications such as graph analytics is rapidly growing with the rise of Big Data. However, parallel graph workloads tend to perform poorly on general-purpose chip multiprocessors (CMPs) due to poor cache locality, low compute intensity, frequent synchronization, uneven task sizes, and dynamic task generation. At high thread counts, execution time is dominated by worklist synchronization overhead and cache misses. Researchers have proposed hardware worklist accelerators to address scheduling costs, but these proposals often harden a specific scheduling policy and do not address high cache miss rates. We address this with Minnow, a technique that augments each core in a CMP with a lightweight Minnow accelerator. Minnow engines offload worklist scheduling from worker threads to improve scalability. The engines also perform worklist-directed prefetching, a technique that exploits knowledge of upcoming tasks to issue nearly perfectly accurate and timely prefetch operations. On a simulated 64-core CMP running a parallel graph benchmark suite, Minnow improves scalability and reduces L2 cache misses from 29 to 1.2 MPKI on average, resulting in 6.01x average speedup over an optimized software baseline for only 1\% area overhead.},
	language = {en},
	number = {2},
	urldate = {2023-02-02},
	journal = {ACM SIGPLAN Notices},
	author = {Zhang, Dan and Ma, Xiaoyu and Thomson, Michael and Chiou, Derek},
	month = nov,
	year = {2018},
	pages = {593--607},
}

@inproceedings{leiserson_work-efficient_2010,
	address = {Thira Santorini Greece},
	title = {A work-efficient parallel breadth-first search algorithm (or how to cope with the nondeterminism of reducers)},
	isbn = {978-1-4503-0079-7},
	url = {https://dl.acm.org/doi/10.1145/1810479.1810534},
	doi = {10.1145/1810479.1810534},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {Proceedings of the twenty-second annual {ACM} symposium on {Parallelism} in algorithms and architectures},
	publisher = {ACM},
	author = {Leiserson, Charles E. and Schardl, Tao B.},
	month = jun,
	year = {2010},
	pages = {303--314},
}

@inproceedings{lattner_mlir_2021,
	address = {Seoul, Korea (South)},
	title = {{MLIR}: {Scaling} {Compiler} {Infrastructure} for {Domain} {Specific} {Computation}},
	isbn = {978-1-72818-613-9},
	shorttitle = {{MLIR}},
	url = {https://ieeexplore.ieee.org/document/9370308/},
	doi = {10.1109/CGO51591.2021.9370308},
	urldate = {2022-09-27},
	booktitle = {2021 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	publisher = {IEEE},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	month = feb,
	year = {2021},
	pages = {2--14},
}

@article{shun_ligra_2013,
	title = {Ligra: a lightweight graph processing framework for shared memory},
	volume = {48},
	issn = {0362-1340, 1558-1160},
	shorttitle = {Ligra},
	url = {https://dl.acm.org/doi/10.1145/2517327.2442530},
	doi = {10.1145/2517327.2442530},
	abstract = {There has been significant recent interest in parallel frameworks for processing graphs due to their applicability in studying social networks, the Web graph, networks in biology, and unstructured meshes in scientific simulation. Due to the desire to process large graphs, these systems have emphasized the ability to run on distributed memory machines. Today, however, a single multicore server can support more than a terabyte of memory, which can fit graphs with tens or even hundreds of billions of edges. Furthermore, for graph algorithms, shared-memory multicores are generally significantly more efficient on a per core, per dollar, and per joule basis than distributed memory systems, and shared-memory algorithms tend to be simpler than their distributed counterparts.
            In this paper, we present a lightweight graph processing framework that is specific for shared-memory parallel/multicore machines, which makes graph traversal algorithms easy to write. The framework has two very simple routines, one for mapping over edges and one for mapping over vertices. Our routines can be applied to any subset of the vertices, which makes the framework useful for many graph traversal algorithms that operate on subsets of the vertices. Based on recent ideas used in a very fast algorithm for breadth-first search (BFS), our routines automatically adapt to the density of vertex sets. We implement several algorithms in this framework, including BFS, graph radii estimation, graph connectivity, betweenness centrality, PageRank and single-source shortest paths. Our algorithms expressed using this framework are very simple and concise, and perform almost as well as highly optimized code. Furthermore, they get good speedups on a 40-core machine and are significantly more efficient than previously reported results using graph frameworks on machines with many more cores.},
	language = {en},
	number = {8},
	urldate = {2023-01-31},
	journal = {ACM SIGPLAN Notices},
	author = {Shun, Julian and Blelloch, Guy E.},
	month = aug,
	year = {2013},
	pages = {135--146},
}

@article{zhang_graphit_2018,
	title = {{GraphIt}: a high-performance graph {DSL}},
	volume = {2},
	issn = {2475-1421},
	shorttitle = {{GraphIt}},
	url = {https://dl.acm.org/doi/10.1145/3276491},
	doi = {10.1145/3276491},
	abstract = {The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. As a result, programmers must try different combinations of a large set of techniques, which make tradeoffs among locality, work-efficiency, and parallelism, to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks and domain specific languages (DSLs) lack flexibility, supporting only a limited set of optimizations.
            This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The algorithm language simplifies expressing the algorithms, while exposing opportunities for optimizations. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of edge traversal, vertex data layout, and program structure optimizations. The separation of algorithm and schedule also enables us to build an autotuner on top of GraphIt to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. We evaluate GraphIt’s performance with seven algorithms on graphs with different structures and sizes. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8×, and is never more than 43\% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.},
	language = {en},
	number = {OOPSLA},
	urldate = {2023-01-31},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Zhang, Yunming and Yang, Mengjiao and Baghdadi, Riyadh and Kamil, Shoaib and Shun, Julian and Amarasinghe, Saman},
	month = oct,
	year = {2018},
	pages = {1--30},
}

@inproceedings{dennis_preliminary_1975,
	address = {Not Known},
	title = {A preliminary architecture for a basic data-flow processor},
	url = {http://portal.acm.org/citation.cfm?doid=642089.642111},
	doi = {10.1145/642089.642111},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the 2nd annual symposium on {Computer} architecture  - {ISCA} '75},
	publisher = {ACM Press},
	author = {Dennis, Jack B. and Misunas, David P.},
	year = {1975},
	pages = {126--132},
}

@unpublished{sotiropoulos_2023_nodate-10,
	title = {2023 w04},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{dahlgren_fixed_1993,
	address = {Syracuse, NY, USA},
	title = {Fixed and {Adaptive} {Sequential} {Prefetching} in {Shared} {Memory} {Multiprocessors}},
	isbn = {978-0-8493-8983-2},
	url = {http://ieeexplore.ieee.org/document/4134114/},
	doi = {10.1109/ICPP.1993.92},
	urldate = {2023-01-27},
	booktitle = {1993 {International} {Conference} on {Parallel} {Processing} - {ICPP}'93 {Vol1}},
	publisher = {IEEE},
	author = {Dahlgren, Fredrik and Dubois, Michel and Stenstrom, Per},
	month = aug,
	year = {1993},
	pages = {56--63},
}

@article{lee_prefetch-aware_2011,
	title = {Prefetch-{Aware} {Memory} {Controllers}},
	volume = {60},
	issn = {0018-9340},
	url = {http://ieeexplore.ieee.org/document/6008537/},
	doi = {10.1109/TC.2010.214},
	number = {10},
	urldate = {2023-01-27},
	journal = {IEEE Transactions on Computers},
	author = {Lee, Chang Joo and Mutlu, Onur and Narasiman, Veynu and Patt, Yale N.},
	month = oct,
	year = {2011},
	pages = {1406--1430},
}

@article{seshadri_mitigating_2015,
	title = {Mitigating {Prefetcher}-{Caused} {Pollution} {Using} {Informed} {Caching} {Policies} for {Prefetched} {Blocks}},
	volume = {11},
	issn = {1544-3566, 1544-3973},
	url = {https://dl.acm.org/doi/10.1145/2677956},
	doi = {10.1145/2677956},
	abstract = {Many modern high-performance processors prefetch blocks into the on-chip cache. Prefetched blocks can potentially pollute the cache by evicting more useful blocks. In this work, we observe that both accurate and inaccurate prefetches lead to cache pollution, and propose a comprehensive mechanism to mitigate prefetcher-caused cache pollution.
            First, we observe that over 95\% of useful prefetches in a wide variety of applications are not reused after the first demand hit (in secondary caches). Based on this observation, our first mechanism simply demotes a prefetched block to the lowest priority on a demand hit. Second, to address pollution caused by inaccurate prefetches, we propose a self-tuning prefetch accuracy predictor to predict if a prefetch is accurate or inaccurate. Only predicted-accurate prefetches are inserted into the cache with a high priority.
            Evaluations show that our final mechanism, which combines these two ideas, significantly improves performance compared to both the baseline LRU policy and two state-of-the-art approaches to mitigating prefetcher-caused cache pollution (up to 49\%, and 6\% on average for 157 two-core multiprogrammed workloads). The performance improvement is consistent across a wide variety of system configurations.},
	language = {en},
	number = {4},
	urldate = {2023-01-27},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Seshadri, Vivek and Yedkar, Samihan and Xin, Hongyi and Mutlu, Onur and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.},
	month = jan,
	year = {2015},
	pages = {1--22},
}

@inproceedings{sharma_data-aware_2022,
	address = {Antwerp, Belgium},
	title = {Data-{Aware} {Cache} {Management} for {Graph} {Analytics}},
	isbn = {978-3-9819263-6-1},
	url = {https://ieeexplore.ieee.org/document/9774709/},
	doi = {10.23919/DATE54114.2022.9774709},
	urldate = {2023-01-26},
	booktitle = {2022 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	publisher = {IEEE},
	author = {Sharma, Neelam and Venkitaraman, Varun and {Newton} and Kumar, Vikash and Singhania, Shubham and Jha, Chandan Kumar},
	month = mar,
	year = {2022},
	pages = {843--848},
}

@inproceedings{jamilan_apt-get_2022,
	address = {Rennes France},
	title = {{APT}-{GET}: profile-guided timely software prefetching},
	isbn = {978-1-4503-9162-7},
	shorttitle = {{APT}-{GET}},
	url = {https://dl.acm.org/doi/10.1145/3492321.3519583},
	doi = {10.1145/3492321.3519583},
	language = {en},
	urldate = {2023-01-26},
	booktitle = {Proceedings of the {Seventeenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Jamilan, Saba and Khan, Tanvir Ahmed and Ayers, Grant and Kasikci, Baris and Litz, Heiner},
	month = mar,
	year = {2022},
	pages = {747--764},
}

@inproceedings{addisie_heterogeneous_2018,
	address = {Raleigh, NC},
	title = {Heterogeneous {Memory} {Subsystem} for {Natural} {Graph} {Analytics}},
	isbn = {978-1-5386-6780-4},
	url = {https://ieeexplore.ieee.org/document/8573480/},
	doi = {10.1109/IISWC.2018.8573480},
	urldate = {2023-01-26},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Addisie, Abraham and Kassa, Hiwot and Matthews, Opeoluwa and Bertacco, Valeria},
	month = sep,
	year = {2018},
	pages = {134--145},
}

@inproceedings{balaji_when_2018,
	address = {Raleigh, NC},
	title = {When is {Graph} {Reordering} an {Optimization}? {Studying} the {Effect} of {Lightweight} {Graph} {Reordering} {Across} {Applications} and {Input} {Graphs}},
	isbn = {978-1-5386-6780-4},
	shorttitle = {When is {Graph} {Reordering} an {Optimization}?},
	url = {https://ieeexplore.ieee.org/document/8573478/},
	doi = {10.1109/IISWC.2018.8573478},
	urldate = {2023-01-24},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Balaji, Vignesh and Lucia, Brandon},
	month = sep,
	year = {2018},
	pages = {203--214},
}

@inproceedings{esmaeilzadeh_dark_2011,
	address = {San Jose California USA},
	title = {Dark silicon and the end of multicore scaling},
	isbn = {978-1-4503-0472-6},
	url = {https://dl.acm.org/doi/10.1145/2000064.2000108},
	doi = {10.1145/2000064.2000108},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Proceedings of the 38th annual international symposium on {Computer} architecture},
	publisher = {ACM},
	author = {Esmaeilzadeh, Hadi and Blem, Emily and St. Amant, Renee and Sankaralingam, Karthikeyan and Burger, Doug},
	month = jun,
	year = {2011},
	pages = {365--376},
}

@inproceedings{yu_imp_2015,
	address = {Waikiki Hawaii},
	title = {{IMP}: indirect memory prefetcher},
	isbn = {978-1-4503-4034-2},
	shorttitle = {{IMP}},
	url = {https://dl.acm.org/doi/10.1145/2830772.2830807},
	doi = {10.1145/2830772.2830807},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Proceedings of the 48th {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Yu, Xiangyao and Hughes, Christopher J. and Satish, Nadathur and Devadas, Srinivas},
	month = dec,
	year = {2015},
	pages = {178--190},
}

@inproceedings{peled_semantic_2015,
	address = {Portland Oregon},
	title = {Semantic locality and context-based prefetching using reinforcement learning},
	isbn = {978-1-4503-3402-0},
	url = {https://dl.acm.org/doi/10.1145/2749469.2749473},
	doi = {10.1145/2749469.2749473},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Peled, Leeor and Mannor, Shie and Weiser, Uri and Etsion, Yoav},
	month = jun,
	year = {2015},
	pages = {285--297},
}

@inproceedings{bera_pythia_2021,
	address = {Virtual Event Greece},
	title = {Pythia: {A} {Customizable} {Hardware} {Prefetching} {Framework} {Using} {Online} {Reinforcement} {Learning}},
	isbn = {978-1-4503-8557-2},
	shorttitle = {Pythia},
	url = {https://dl.acm.org/doi/10.1145/3466752.3480114},
	doi = {10.1145/3466752.3480114},
	language = {en},
	urldate = {2023-01-24},
	booktitle = {{MICRO}-54: 54th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Bera, Rahul and Kanellopoulos, Konstantinos and Nori, Anant and Shahroodi, Taha and Subramoney, Sreenivas and Mutlu, Onur},
	month = oct,
	year = {2021},
	pages = {1121--1137},
}

@unpublished{sotiropoulos_2023_nodate-11,
	title = {2023 w03},
	author = {Sotiropoulos, Konstantinos},
}

@article{horowitz_informing_1998,
	title = {Informing {Memory} {Operations}: {Providing} {Memory} {Performance} {Feedback} in {Modern} {Processors}},
	volume = {16},
	issn = {0734-2071, 1557-7333},
	shorttitle = {Informing memory operations},
	url = {https://dl.acm.org/doi/10.1145/279227.279230},
	doi = {10.1145/279227.279230},
	abstract = {Memory latency is an important bottleneck in system performance that cannot be adequately solved by hardware alone. Several promising software techniques have been shown to address this problem successfully in specific situations. However, the generality of these software approaches has been limited because current architecturtes do not provide a fine-grained, low-overhead mechanism for observing and reacting to memory behavior directly. To fill this need, this article proposes a new class of memory operations called
              informing memory operations
              , which essentially consist of a memory operatin combined (either implicitly or explicitly) with a conditional branch-and-ink operation that is taken only if the reference suffers a cache miss. This article describes two different implementations of informing memory operations. One is based on a
              cache-outcome condition code,
              and the other is based on
              low-overhead traps.
              We find that modern in-order-issue and out-of-order-issue superscalar processors already contain the bulk of the necessary hardware support. We describe how a number of software-based memory optimizations can exploit informing memory operations to enhance performance, and we look at cache coherence with fine-grained access control as a case study. Our performance results demonstrate that the runtime overhead of invoking the informing mechanism on the Alpha 21164 and MIPS R10000 processors is generally small enough to provide considerable flexibility to hardware and software designers, and that the cache coherence application has improved performance compared to other current solutions. We believe that the inclusion of informing memory operations in future processors may spur even more innovative performance optimizations.},
	language = {en},
	number = {2},
	urldate = {2022-09-11},
	journal = {ACM Transactions on Computer Systems},
	author = {Horowitz, Mark and Martonosi, Margaret and Mowry, Todd C. and Smith, Michael D.},
	month = may,
	year = {1998},
	pages = {170--205},
}

@article{mittal_survey_2017,
	title = {A {Survey} of {Recent} {Prefetching} {Techniques} for {Processor} {Caches}},
	volume = {49},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/2907071},
	doi = {10.1145/2907071},
	abstract = {As the trends of process scaling make memory systems an even more crucial bottleneck, the importance of latency hiding techniques such as prefetching grows further. However, naively using prefetching can harm performance and energy efficiency and, hence, several factors and parameters need to be taken into account to fully realize its potential. In this article, we survey several recent techniques that aim to improve the implementation and effectiveness of prefetching. We characterize the techniques on several parameters to highlight their similarities and differences. The aim of this survey is to provide insights to researchers into working of prefetching techniques and spark interesting future work for improving the performance advantages of prefetching even further.},
	language = {en},
	number = {2},
	urldate = {2022-12-02},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh},
	month = jun,
	year = {2017},
	keywords = {survey},
	pages = {1--35},
}

@inproceedings{atta_self-contained_2015,
	address = {Waikiki Hawaii},
	title = {Self-contained, accurate precomputation prefetching},
	isbn = {978-1-4503-4034-2},
	url = {https://dl.acm.org/doi/10.1145/2830772.2830816},
	doi = {10.1145/2830772.2830816},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 48th {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Atta, Islam and Tong, Xin and Srinivasan, Vijayalakshmi and Baldini, Ioana and Moshovos, Andreas},
	month = dec,
	year = {2015},
	pages = {153--165},
}

@inproceedings{mehta_multi-stage_2014,
	address = {Munich Germany},
	title = {Multi-stage coordinated prefetching for present-day processors},
	isbn = {978-1-4503-2642-1},
	url = {https://dl.acm.org/doi/10.1145/2597652.2597660},
	doi = {10.1145/2597652.2597660},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 28th {ACM} international conference on {Supercomputing}},
	publisher = {ACM},
	author = {Mehta, Sanyam and Fang, Zhenman and Zhai, Antonia and Yew, Pen-Chung},
	month = jun,
	year = {2014},
	pages = {73--82},
}

@inproceedings{litz_crisp_2022,
	address = {Lausanne Switzerland},
	title = {{CRISP}: critical slice prefetching},
	isbn = {978-1-4503-9205-1},
	shorttitle = {{CRISP}},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507745},
	doi = {10.1145/3503222.3507745},
	language = {en},
	urldate = {2023-01-18},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Litz, Heiner and Ayers, Grant and Ranganathan, Parthasarathy},
	month = feb,
	year = {2022},
	pages = {300--313},
}

@unpublished{sotiropoulos_2022_nodate,
	title = {2022 w49},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2023_nodate-12,
	title = {2023 w02},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_w48_follow_up_presentation_nodate,
	title = {2022\_w48\_follow\_up\_presentation},
	author = {Sotiropoulos, Konstantinos},
}

@article{naderan-tahan_why_2016,
	title = {Why {Does} {Data} {Prefetching} {Not} {Work} for {Modern} {Workloads}?},
	volume = {59},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxv112},
	doi = {10.1093/comjnl/bxv112},
	language = {en},
	number = {2},
	urldate = {2022-12-06},
	journal = {The Computer Journal},
	author = {Naderan-Tahan, Mahmood and Sarbazi-Azad, Hamid},
	month = feb,
	year = {2016},
	pages = {244--259},
}

@misc{bakhshalipour_survey_2020,
	title = {A {Survey} on {Recent} {Hardware} {Data} {Prefetching} {Approaches} with {An} {Emphasis} on {Servers}},
	url = {http://arxiv.org/abs/2009.00715},
	abstract = {Data prefetching, i.e., the act of predicting application's future memory accesses and fetching those that are not in the on-chip caches, is a well-known and widely-used approach to hide the long latency of memory accesses. The fruitfulness of data prefetching is evident to both industry and academy: nowadays, almost every high-performance processor incorporates a few data prefetchers for capturing various access patterns of applications; besides, there is a myriad of proposals for data prefetching in the research literature, where each proposal enhances the efficiency of prefetching in a specific way. In this survey, we discuss the fundamental concepts in data prefetching and study state-of-the-art hardware data prefetching approaches. Additional Key Words and Phrases: Data Prefetching, Scale-Out Workloads, Server Processors, and Spatio-Temporal Correlation.},
	urldate = {2022-12-02},
	publisher = {arXiv},
	author = {Bakhshalipour, Mohammad and Shakerinava, Mehran and Golshan, Fatemeh and Ansari, Ali and Lotfi-Karman, Pejman and Sarbazi-Azad, Hamid},
	month = sep,
	year = {2020},
	note = {arXiv:2009.00715 [cs]},
	keywords = {Computer Science - Hardware Architecture},
}

@unpublished{sotiropoulos_2022_nodate-1,
	title = {2022 w48},
	author = {Sotiropoulos, Konstantinos},
}

@book{salomon_variable-length_2007,
	address = {London},
	title = {Variable-length {Codes} for {Data} {Compression}},
	isbn = {978-1-84628-958-3 978-1-84628-959-0},
	url = {http://link.springer.com/10.1007/978-1-84628-959-0},
	language = {en},
	urldate = {2022-11-30},
	publisher = {Springer London},
	author = {Salomon, David},
	year = {2007},
	doi = {10.1007/978-1-84628-959-0},
}

@inproceedings{arelakis_sc2_2014,
	address = {Minneapolis, MN, USA},
	title = {{SC}$^{\textrm{2}}$: {A} statistical compression cache scheme},
	isbn = {978-1-4799-4394-4 978-1-4799-4396-8},
	shorttitle = {{SC}$^{\textrm{2}}$},
	url = {http://ieeexplore.ieee.org/document/6853231/},
	doi = {10.1109/ISCA.2014.6853231},
	urldate = {2022-11-29},
	booktitle = {2014 {ACM}/{IEEE} 41st {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Arelakis, Angelos and Stenstrom, Per},
	month = jun,
	year = {2014},
	pages = {145--156},
}

@article{beamer_gap_2015,
	title = {The {GAP} {Benchmark} {Suite}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1508.03619},
	doi = {10.48550/ARXIV.1508.03619},
	abstract = {We present a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.},
	urldate = {2022-11-27},
	author = {Beamer, Scott and Asanović, Krste and Patterson, David},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {Data Structures and Algorithms (cs.DS), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences},
}

@article{boisvert_incentivizing_2016,
	title = {Incentivizing reproducibility},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2994031},
	doi = {10.1145/2994031},
	language = {en},
	number = {10},
	urldate = {2022-11-25},
	journal = {Communications of the ACM},
	author = {Boisvert, Ronald F.},
	month = sep,
	year = {2016},
	pages = {5--5},
}

@unpublished{sotiropoulos_2022_nodate-2,
	title = {2022 w47},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-3,
	title = {2022 w46},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-4,
	title = {2022 w45},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{kroft_lockup-free_1998,
	address = {Barcelona, Spain},
	title = {Lockup-free instruction fetch/prefetch cache organization},
	isbn = {978-1-58113-058-4},
	url = {http://portal.acm.org/citation.cfm?doid=285930.285979},
	doi = {10.1145/285930.285979},
	language = {en},
	urldate = {2022-11-16},
	booktitle = {25 years of the international symposia on {Computer} architecture (selected papers)  - {ISCA} '98},
	publisher = {ACM Press},
	author = {Kroft, David},
	year = {1998},
	pages = {195--201},
}

@inproceedings{chen_prefetching_2008,
	address = {Boston, MA, USA},
	title = {Prefetching irregular references for software cache on cell},
	isbn = {978-1-59593-978-4},
	url = {http://portal.acm.org/citation.cfm?doid=1356058.1356079},
	doi = {10.1145/1356058.1356079},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the sixth annual {IEEE}/{ACM} international symposium on {Code} generation and optimization  - {CGO} '08},
	publisher = {ACM Press},
	author = {Chen, Tong and Zhang, Tao and Sura, Zehra and Tallada, Mar Gonzales},
	year = {2008},
	pages = {155},
}

@unpublished{sotiropoulos_2022_nodate-5,
	title = {2022 w43},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-6,
	title = {2022 w42},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-7,
	title = {2022 w41},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-8,
	title = {2022 w40},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-9,
	title = {2022 w39},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-10,
	title = {2022 w38},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-11,
	title = {2022 w37},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate-12,
	title = {2022 w36},
	author = {Sotiropoulos, Konstantinos},
}

@unpublished{sotiropoulos_2022_nodate,
	title = {2022 w35},
	author = {Sotiropoulos, Konstantinos},
}

@inproceedings{tsai_rethinking_2018,
	address = {Fukuoka},
	title = {Rethinking the {Memory} {Hierarchy} for {Modern} {Languages}},
	isbn = {978-1-5386-6240-3},
	url = {https://ieeexplore.ieee.org/document/8574542/},
	doi = {10.1109/MICRO.2018.00025},
	urldate = {2022-10-13},
	booktitle = {2018 51st {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	publisher = {IEEE},
	author = {Tsai, Po-An and Gan, Yee Ling and Sanchez, Daniel},
	month = oct,
	year = {2018},
	pages = {203--216},
}

@inproceedings{tsai_compress_2019,
	address = {Providence RI USA},
	title = {Compress {Objects}, {Not} {Cache} {Lines}: {An} {Object}-{Based} {Compressed} {Memory} {Hierarchy}},
	isbn = {978-1-4503-6240-5},
	shorttitle = {Compress {Objects}, {Not} {Cache} {Lines}},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304006},
	doi = {10.1145/3297858.3304006},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Tsai, Po-An and Sanchez, Daniel},
	month = apr,
	year = {2019},
	pages = {229--242},
}

@article{leiserson_theres_2020,
	title = {There’s plenty of room at the {Top}: {What} will drive computer performance after {Moore}’s law?},
	volume = {368},
	issn = {0036-8075, 1095-9203},
	shorttitle = {There’s plenty of room at the {Top}},
	url = {https://www.science.org/doi/10.1126/science.aam9744},
	doi = {10.1126/science.aam9744},
	abstract = {From bottom to top
            
              The doubling of the number of transistors on a chip every 2 years, a seemly inevitable trend that has been called Moore's law, has contributed immensely to improvements in computer performance. However, silicon-based transistors cannot get much smaller than they are today, and other approaches should be explored to keep performance growing. Leiserson
              et al.
              review recent examples and argue that the most promising place to look is at the top of the computing stack, where improvements in software, algorithms, and hardware architecture can bring the much-needed boost.
            
            
              Science
              , this issue p.
              eaam9744
            
          , 
            
              BACKGROUND
              Improvements in computing power can claim a large share of the credit for many of the things that we take for granted in our modern lives: cellphones that are more powerful than room-sized computers from 25 years ago, internet access for nearly half the world, and drug discoveries enabled by powerful supercomputers. Society has come to rely on computers whose performance increases exponentially over time.
              Much of the improvement in computer performance comes from decades of miniaturization of computer components, a trend that was foreseen by the Nobel Prize–winning physicist Richard Feynman in his 1959 address, “There’s Plenty of Room at the Bottom,” to the American Physical Society. In 1975, Intel founder Gordon Moore predicted the regularity of this miniaturization trend, now called Moore’s law, which, until recently, doubled the number of transistors on computer chips every 2 years.
              Unfortunately, semiconductor miniaturization is running out of steam as a viable way to grow computer performance—there isn’t much more room at the “Bottom.” If growth in computing power stalls, practically all industries will face challenges to their productivity. Nevertheless, opportunities for growth in computing performance will still be available, especially at the “Top” of the computing-technology stack: software, algorithms, and hardware architecture.
            
            
              ADVANCES
              Software can be made more efficient by performance engineering: restructuring software to make it run faster. Performance engineering can remove inefficiencies in programs, known as software bloat, arising from traditional software-development strategies that aim to minimize an application’s development time rather than the time it takes to run. Performance engineering can also tailor software to the hardware on which it runs, for example, to take advantage of parallel processors and vector units.
              Algorithms offer more-efficient ways to solve problems. Indeed, since the late 1970s, the time to solve the maximum-flow problem improved nearly as much from algorithmic advances as from hardware speedups. But progress on a given algorithmic problem occurs unevenly and sporadically and must ultimately face diminishing returns. As such, we see the biggest benefits coming from algorithms for new problem domains (e.g., machine learning) and from developing new theoretical machine models that better reflect emerging hardware.
              Hardware architectures can be streamlined—for instance, through processor simplification, where a complex processing core is replaced with a simpler core that requires fewer transistors. The freed-up transistor budget can then be redeployed in other ways—for example, by increasing the number of processor cores running in parallel, which can lead to large efficiency gains for problems that can exploit parallelism. Another form of streamlining is domain specialization, where hardware is customized for a particular application domain. This type of specialization jettisons processor functionality that is not needed for the domain. It can also allow more customization to the specific characteristics of the domain, for instance, by decreasing floating-point precision for machine-learning applications.
              In the post-Moore era, performance improvements from software, algorithms, and hardware architecture will increasingly require concurrent changes across other levels of the stack. These changes will be easier to implement, from engineering-management and economic points of view, if they occur within big system components: reusable software with typically more than a million lines of code or hardware of comparable complexity. When a single organization or company controls a big component, modularity can be more easily reengineered to obtain performance gains. Moreover, costs and benefits can be pooled so that important but costly changes in one part of the big component can be justified by benefits elsewhere in the same component.
            
            
              OUTLOOK
              As miniaturization wanes, the silicon-fabrication improvements at the Bottom will no longer provide the predictable, broad-based gains in computer performance that society has enjoyed for more than 50 years. Software performance engineering, development of algorithms, and hardware streamlining at the Top can continue to make computer applications faster in the post-Moore era. Unlike the historical gains at the Bottom, however, gains at the Top will be opportunistic, uneven, and sporadic. Moreover, they will be subject to diminishing returns as specific computations become better explored.
              
                
                  Performance gains after Moore’s law ends.
                  In the post-Moore era, improvements in computing power will increasingly come from technologies at the “Top” of the computing stack, not from those at the “Bottom”, reversing the historical trend.
                
                
                
                  CREDIT: N. CARY/
                  SCIENCE
                
              
            
          , 
            The miniaturization of semiconductor transistors has driven the growth in computer performance for more than 50 years. As miniaturization approaches its limits, bringing an end to Moore’s law, performance gains will need to come from software, algorithms, and hardware. We refer to these technologies as the “Top” of the computing stack to distinguish them from the traditional technologies at the “Bottom”: semiconductor physics and silicon-fabrication technology. In the post-Moore era, the Top will provide substantial performance gains, but these gains will be opportunistic, uneven, and sporadic, and they will suffer from the law of diminishing returns. Big system components offer a promising context for tackling the challenges of working at the Top.},
	language = {en},
	number = {6495},
	urldate = {2022-10-02},
	journal = {Science},
	author = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
	month = jun,
	year = {2020},
	pages = {eaam9744},
}

@misc{intel_introduction_nodate,
	title = {Introduction to the {iAPX} 432 {Architecture}},
	author = {Intel},
}

@misc{arm_morello_nodate,
	title = {Morello {Prototype} {Architecture} {Overview}},
	author = {ARM},
}

@inproceedings{hallnor_fully_2000,
	address = {Vancouver, British Columbia, Canada},
	title = {A fully associative software-managed cache design},
	isbn = {978-1-58113-232-8},
	url = {http://portal.acm.org/citation.cfm?doid=339647.339660},
	doi = {10.1145/339647.339660},
	language = {en},
	urldate = {2022-09-23},
	booktitle = {Proceedings of the 27th annual international symposium on {Computer} architecture  - {ISCA} '00},
	publisher = {ACM Press},
	author = {Hallnor, Erik G. and Reinhardt, Steven K.},
	year = {2000},
	pages = {107--116},
}

@inproceedings{schwedock_tako_2022,
	address = {New York, NY, USA},
	series = {{ISCA} '22},
	title = {täkō: a polymorphic cache hierarchy for general-purpose optimization of data movement},
	isbn = {978-1-4503-8610-4},
	shorttitle = {täkō},
	url = {https://doi.org/10.1145/3470496.3527379},
	doi = {10.1145/3470496.3527379},
	abstract = {Current systems hide data movement from software behind the load-store interface. Software's inability to observe and respond to data movement is the root cause of many inefficiencies, including the growing fraction of execution time and energy devoted to data movement itself. Recent specialized memory-hierarchy designs prove that large data-movement savings are possible. However, these designs require custom hardware, raising a large barrier to their practical adoption. This paper argues that the hardware-software interface is the problem, and custom hardware is often unnecessary with an expanded interface. The täkō architecture lets software observe data movement and interpose when desired. Specifically, caches in täkō can trigger software callbacks in response to misses, evictions, and writebacks. Callbacks run on reconfigurable dataflow engines placed near caches. Five case studies show that this interface covers a wide range of data-movement features and optimizations. Microarchitecturally, täkō is similar to recent near-data computing designs, adding ≈5\% area to a baseline multicore. täkō improves performance by 1.4X-4.2X, similar to prior custom hardware designs, and comes within 1.8\% of an idealized implementation.},
	urldate = {2022-08-29},
	booktitle = {Proceedings of the 49th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Schwedock, Brian C. and Yoovidhya, Piratach and Seibert, Jennifer and Beckmann, Nathan},
	month = jun,
	year = {2022},
	keywords = {cache hierarchy, data movement, data-centric computing},
	pages = {42--58},
}

@inproceedings{caminal_cape_2021,
	title = {{CAPE}: {A} {Content}-{Addressable} {Processing} {Engine}},
	shorttitle = {{CAPE}},
	doi = {10.1109/HPCA51647.2021.00054},
	abstract = {Processing-in-memory (PIM) architectures attempt to overcome the von Neumann bottleneck by combining computation and storage logic into a single component. The content-addressable parallel processing paradigm (CAPP) from the seventies is an in-situ PIM architecture that leverages content-addressable memories to realize bit-serial arithmetic and logic operations, via sequences of search and update operations over multiple memory rows in parallel. In this paper, we set out to investigate whether the concepts behind classic CAPP can be used successfully to build an entirely CMOS-based, general-purpose microarchitecture that can deliver manyfold speedups while remaining highly programmable. We conduct a full-stack design of a Content-Addressable Processing Engine (CAPE), built out of dense push-rule 6T SRAM arrays. CAPE is programmable using the RISC-V ISA with standard vector extensions. Our experiments show that CAPE achieves an average speedup of 14 (up to 254) over an area-equivalent (slightly under 9 mm2 at 7 nm) out-of-order processor core with three levels of caches.},
	booktitle = {2021 {IEEE} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture} ({HPCA})},
	author = {Caminal, Helena and Yang, Kailin and Srinivasa, Srivatsa and Ramanathan, Akshay Krishna and Al-Hawaj, Khalid and Wu, Tianshu and Narayanan, Vijaykrishnan and Batten, Christopher and Martínez, José F.},
	month = feb,
	year = {2021},
	note = {ISSN: 2378-203X},
	keywords = {Associative processing, Computer architecture, Engines, Microarchitecture, Out of order, Parallel processing, Random access memory, associative memory, vector processors},
	pages = {557--569},
}

@inproceedings{lockerman_livia_2020,
	address = {New York, NY, USA},
	series = {{ASPLOS} '20},
	title = {Livia: {Data}-{Centric} {Computing} {Throughout} the {Memory} {Hierarchy}},
	isbn = {978-1-4503-7102-5},
	shorttitle = {Livia},
	url = {https://doi.org/10.1145/3373376.3378497},
	doi = {10.1145/3373376.3378497},
	abstract = {In order to scale, future systems will need to dramatically reduce data movement. Data movement is expensive in current designs because (i) traditional memory hierarchies force computation to happen unnecessarily far away from data and (ii) processing-in-memory approaches fail to exploit locality. We propose Memory Services, a flexible programming model that enables data-centric computing throughout the memory hierarchy. In Memory Services, applications express functionality as graphs of simple tasks, each task indicating the data it operates on. We design and evaluate Livia, a new system architecture for Memory Services that dynamically schedules tasks and data at the location in the memory hierarchy that minimizes overall data movement. Livia adds less than 3\% area overhead to a tiled multicore and accelerates challenging irregular workloads by 1.3 × to 2.4 × while reducing dynamic energy by 1.2× to 4.7×.},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lockerman, Elliot and Feldmann, Axel and Bakhshalipour, Mohammad and Stanescu, Alexandru and Gupta, Shashwat and Sanchez, Daniel and Beckmann, Nathan},
	month = mar,
	year = {2020},
	keywords = {cache, memory, near-data processing},
	pages = {417--433},
}

@article{williams_roofline_2009,
	title = {Roofline: an insightful visual performance model for multicore architectures},
	volume = {52},
	issn = {0001-0782},
	shorttitle = {Roofline},
	url = {https://doi.org/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
	number = {4},
	urldate = {2022-08-25},
	journal = {Communications of the ACM},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = apr,
	year = {2009},
	pages = {65--76},
}

@article{wright_object-aware_2006,
	title = {An object-aware memory architecture},
	volume = {62},
	issn = {01676423},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642306000797},
	doi = {10.1016/j.scico.2006.02.007},
	language = {en},
	number = {2},
	urldate = {2022-09-16},
	journal = {Science of Computer Programming},
	author = {Wright, Greg and Seidl, Matthew L. and Wolczko, Mario},
	month = oct,
	year = {2006},
	pages = {145--163},
}
