\documentclass{acaces}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{minted}

\lstdefinestyle{cppcode}{
    language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{purple},
    frame=none,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt
}

\begin{document}

\title{DPREF: Decoupling using Dead Code Elimination for Prefetching Irregular Memory Accesses}

\author{
Konstantinos~Sotiropoulos\addressnum{1}\comma\extranum{1},
Jonas~Skeppstedt\addressnum{2}\comma\extranum{2},
Angelos~Arelakis\addressnum{3}\comma\extranum{3},
Per~Stenstr\"{o}m\addressnum{1}\comma\extranum{1}
}

\address{1}{
  Chalmers University of Technology
}

\address{2}{
  Lund University
}

\address{3}{
  ZeroPoint Technologies
}

\extra{1}{\{konstantinos.sotirpoulos,per.stenstrom\}@chalmers.se}
\extra{2}{jonas.skeppstedt@cs.lth.se}
\extra{3}{angelos.arelakis@zptcorp.com}

\pagestyle{empty}

\begin{abstract}

  Decoupling refers to the act of slicing a single instruction stream into separate memory accessing and computation streams, and arranging them in a dataflow manner.
  As these instruction streams gain a certain level of autonomy, decoupling can serve as a means to achieve accurate and timely prefetching.

  Previous work on compile-time decoupling has yielded promising results for irregular applications.
  However, the presented slicing methods are limited in terms of their applicability and scalability.

  This research aims to advance previous works on compile-time decoupling by proposing a flexible and systematic approach to slicing.
  The ultimate goal is to broaden the applicability to a wider range of irregular applications that existing methods struggle to address. \\

\end{abstract}

\keywords{irregular applications; decoupling; slicing; prefetching; DCE}

\section{Introduction}

Irregular memory accesses pose a significant challenge in applications within graph analytics, database management systems, and sparse linear algebra.
These applications experience high execution times due to frequent stalls caused by long latency DRAM accesses.
Prefetching is a latency avoidance technique that aims to bring useful data closer to the processor before it is requested.

Processors are equipped with dedicated hardware prefetchers that predict future accesses and do so transparently, without requiring any software intervention.
These prefetchers are efficient for applications with regular access patterns, such as iterating over a dense matrix.
Nonetheless, they struggle with indirect memory accesses, like those encountered when traversing a sparse graph.

There have been several proposals to improve the predictive capabilities of hardware prefetchers e.g. with the application of machine learning \cite{bera_pythia_2021}.
However, due to their speculative nature, these techniques can not be accurate, fetching irrelevant alongside useful data, and thus wasting bandwidth, on-chip space and energy.

To enhance accuracy, another class of proposals aims to add some programmability to hardware prefetchers \cite{talati_prodigy_2021}.
Still, this class of prefetchers has limited applicability, as their constrained programming model is only able to cover certain data structures and representations.
Moreover, programming these prefetchers necessitates the use of a different programming language from the one used to express the application,
creating a learning barrier that hinders the wider adoption of such proposals.

In addition to hardware prefetchers, processor ISAs include specialized instructions to explicitly request data prefetches.
Programmers and compilers can leverage their knowledge of the underlying microarchitecture and an application's access patterns to utilize these instructions.
This technique, known as software prefetching, has a fundamental limitation:
the timing of prefetches, as expressed statically by a look-ahead distance or the location of a prefetch instruction, is hard to reason about.
This limitation becomes particularly evident with irregular applications,
where software prefetching techniques provide modest performance gains \cite{ainsworth_software_2018}.

A recent wave of proposals aims to provide timely and accurate prefetches for irregular applications that use sparse data structures,
demonstrating very promising results \cite{Manocha2021, Nguyen2023}.
Due to the way sparse data structures are represented,
their traversal exhibits a specific type of irregularity, indirection, typically taking the form \mintinline{C}{A[B[C[i]]]}.
The key idea behind these proposals is to decouple such multi-level indirection into distinct pipeline stages, each housing a slice of the application.

These proposals represent examples of hardware-software co-design:
the decoupling is enacted by the compiler, and they offer ISA-visible queues for stage-to-stage communication.
These queues benefit from low-latency hardware implementation as inter-stage communication occurs every few cycles.

Despite their promising results, these existing proposals face limitations due to their slicing methods.
Either they can produce only a restricted number of slices, as in \cite{Manocha2021},
or they can only slice applications that can be arranged as feed-forward pipelines, as in \cite{Nguyen2023}.

The primary purpose of this study is to extend these proposals through a flexible and systematic compiler slicing technique,
utilizing an established optimization technique called Dead Code Elimination (DCE) \cite{cytron_efficiently_1991}.
Paired with an appropriate processor architecture, our approach can handle a broader range of applications while scaling
with Memory Level Parallelism (MLP) and memory bandwidth.

\section{DCE for Slicing}

With DCE, the compiler partitions a program into live and dead code, and then discards the dead portion.
As will be demonstrated, DCE can evolve into a flexible and systematic slicing technique by redefining what is considered as live code.

In standard DCE, live/essential code is formulated on the basis of pre-live statements:
statements that have an effect on observable state, e.g. I/O operations or explicit memory manipulation.
Pre-live statements along with their data- and control- dependencies are what constitutes live code.
Take, for instance, figure \ref{fig:dce}: within the function body, the only pre-live statement is located at line 6.
This statement is data-dependent on line 2 and control-dependent on line 3.

\begin{figure}[ht]
  \centering
  \input{dce.tex}
  \caption{Live code is highlighted; the remaining code is considered dead/ineffectual and can be discarded by an optimizing compiler}
  \label{fig:dce}
\end{figure}

Drawing from the concept of program slicing as introduced by Weiser \cite{Weiser1984}, live code can be viewed as a slice:
a reduced version of the pre-optimized program, guaranteed to produce the intended behavior with respect to a slicing criterion, the pre-live statements.
To perform any arbitrary slicing with DCE, one simply needs to indicate pre-live statements.
This is central to the programmer/compiler interface design,
and can be facilitated through the use of annotations,
as will be discussed in the upcoming section.

\section{DPREF on Sparse Matrix-Vector Multiplication}

Figure \ref{fig:spmv} demonstrates the the product y = Ax of a sparse matrix A and a dense vector x (SpMV) where A is stored in Compressed Sparse Row (CSR) format.
Array \mintinline{C++}{Values} stores all non-zero values of the matrix and array \mintinline{C++}{ColIdx} stores the column index of each non-zero value.
The \mintinline{C++}{Offsets} array denotes the start index of each row in the \mintinline{C++}{Values} and \mintinline{C++}{ColIdx} arrays.
For instance, the non-zero values of row \mintinline{C++}{n} in the \mintinline{C++}{Values} arrays start at index \mintinline{C++}{Offsets[n]}
and end just before index \mintinline{C++}{Offsets[n+1]}.

The \mintinline{C++}{pragma} directive in line (1) explicitly denotes this relationship among the arrays.
The compiler leverages this annotation to mark as pre-live the instructions that use \mintinline{C++}{Values} and \mintinline{C++}{X},
while deliberately excluding their indices from the dependency analysis.
This marks the beginning of the first slicing phase.

Figure \ref{fig:firstSlice} is the slice that corresponds to the dead code.
In this slice, a queue is used to push non-zero values of the matrix and their corresponding column indices.
The queue is not just a conduit for data transfer, but also a carrier of control flow information.
For instance, when processing of a row is completed, a \mintinline{C++}{LoopTerminationToken} is pushed into the queue.
Similarly, when the entire matrix has been processed, a \mintinline{C++}{TerminationToken} is pushed, indicating completion of the task.

Figure \ref{fig:secondSlice} is the slice that corresponds to the live code.
The column loop \mintinline{C++}{J} has been substituted with an infinite loop,
where tokens are popped from the queue.
These tokens are initially checked for control flow information before being utilized for actual computation.
Though not depicted here, this slice can be further sliced on the access to \mintinline{C++}{X[ColIdx]},
reducing its scope to solely performing the computation and storing of the result

\begin{figure}[ht]
  \centering
  \input{SpMV.tex}
  \caption{SpMV, before slicing. Shaded in green is exclusively live code, shaded in red is exclusively dead code, and code in yellow is common to both. }
  \label{fig:spmv}
\end{figure}

\begin{figure}[ht]
  \centering

  \begin{subfigure}{1.0\textwidth}
    \centering
    \input{firstSlice.tex}
    \caption{The first slice corresponding to the dead code}
    \label{fig:firstSlice}
  \end{subfigure}

  \begin{subfigure}{1.0\textwidth}
    \centering
    \input{secondSlice.tex}
    \caption{The second slice corresponding to the live code}
    \label{fig:secondSlice}
  \end{subfigure}

\end{figure}

\section{Preliminary Conclusion}

Our work is currently in progress, exploring the viability of our proposed approach through manual application slicing.
Our study extends beyond the SpMV algorithm, including applications like breadth-first search, PageRank, and Hash Join.
Three primary challenges have emerged from our findings thus far.

First, for algorithms that are not as straightforwardly decouplable as SpMV,
and which encompass feedback loops,
it's essential for the compiler to discern and reason about critical system properties, such as deadlock.
Modeling the system as a Petri net within the compiler might offer a viable solution to this issue.

Second, when a feedback loop exists between the first and final slice, a distributed termination solution is required.
Specifically, the first slice needs to determine when all queues are empty and all slices are inactive so that the termination token can be injected \cite{Dijkstra1983}.

Lastly, the microarchitecture of the processor, apart from facilitating quick push/pop operations, must also handle the overhead associated with checking for control flow tokens.

\section{Acknowledgment}

This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP)
funded by the Knut and Alice Wallenberg Foundation.

\bibliography{main}

\end{document}
