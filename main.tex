\documentclass{acaces}

\begin{document}

\title{Using DCE for slicing applications with irregular memory accesses}

\pagestyle{empty}

\begin{abstract}

\end{abstract}

\keywords{irregular applications; prefetching; pipelined parallelism; DCE}

\section{Introduction}

Irregular memory accesses pose a significant challenge in applications within graph analytics, database management systems, and sparse linear algebra.
These applications experience high execution times due to frequent stalls caused by long latency DRAM accesses.

Prefetching is a latency avoidance technique that aims to bring useful data closer to the processor before it is requested.
Programmers, leveraging their knowledge of the application, can utilize specialized instructions to explicitly request data prefetches.
However, this approach requires significant effort and a thorough understanding of the processor's microarchitecture.
It may also lead to non-portable code.

In addition to prefetching instructions, modern processors are also equipped with dedicated hardware prefetchers.
A hardware prefetcher predicts future accesses and does so transparently, without requiring any software intervention.
These prefetchers are efficient for applications with regular access patterns, such as iterating over a dense matrix.
Nonetheless, they struggle with indirect memory accesses, like those encountered when traversing a sparse graph.

There have been several proposals to improve the predictive capabilities of hardware prefetchers e.g. with the application of machine learning \cite{bera_pythia_2021}.
However, due to their speculative nature, these techniques can not be accurate, fetching irrelevant alongside useful data, and thus wasting bandwidth and energy.

To enhance accuracy, another class of proposals aims to add some programmability to hardware prefetchers \cite{basak_analysis_2019, talati_prodigy_2021, yang_spzip_2021}.
Still, this class of prefetchers has limited applicability, as their constrained programming model is only able to cover certain data structures and representations.
Moreover, programming these prefetchers necessitates the use of a different programming language from the one used to express the application,
creating a learning barrier that hinders the wider adoption of such proposals.

The primary goal of this study is to propose an accurate prefetching scheme with broader applicability.
Rather than introducing additional prefetching hardware, our solution aims to increase the utilization of existing idle resources.
The most important of these resources is the capacity of modern processors for Memory Level Parallelism (MLP),
which refers to their ability to have multiple concurrent memory requests "in-flight".

At the core of our proposal is a transformation of the application into a pipeline similar to \cite{ham_desc_2015, manocha_graphattack_2021, nguyen_phloem_2023}.
This transformation is made feasible by re-purposing a well-established compiler optimization technique called Dead Code Elimination (DCE) \cite{cytron_efficiently_1991}.
In traditional DCE, the compiler slices an application into dead (redundant) and live (essential) code, then discards the redundant portion.
In our approach, the compiler uses the re-purposed DCE to slice the application into memory and compute slices.
The memory slices contain the essential code needed to formulate the memory accesses.
These memory slices are placed in the early stages of the pipeline, designed to maximize MLP as soon as possible.

Our proposal aspires to minimize programmer effort, as the burden of transformation is handled by the compiler.
Moreover, the accuracy and applicability of our technique are assured, as the memory slices are derived from the application itself.
This results in prefetching that closely matches the actual memory access pattern of the application.

\section{Background}

1/2 page

Answer these questions:
\begin{itemize}
\item Why do you want to slice the application?
      To create \textbf{decoupled} (Access/Execute || Supply/Compute || Memory/Compute) threads
\item How does DCE work, why DCE?
      Better than (why?) slicing techniques in similar designs \cite{ham_desc_2015}
\end{itemize}

\section{The "new stuff"} 

2 pages

Answer these questions:
\begin{itemize}
\item What is an irregular access pattern?
      A memory access pattern that exhibits control- and data-dependent memory accesses with poor temporal- or spatial-locallity
\item Can you demonstrate your approach?
      I need to find a kernel that demonstrates all 3 types of LoDs (Loss of Decoupling) events \cite{bird_effectiveness_1993}
      I need to demonstrate the recursive application of DCE.
\end{itemize}

\section{Preliminary conclusion}

1/2 page

Answer these questions:
\begin{itemize}
\item Can you state a preliminary conclusion?
\item What is next, which challenges do you need to address?
\end{itemize}

\bibliography{references}

\end{document}

